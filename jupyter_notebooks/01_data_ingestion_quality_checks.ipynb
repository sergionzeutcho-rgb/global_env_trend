{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e23e447b",
   "metadata": {},
   "source": [
    "# 01 Data Ingestion and Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb6714",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Load the raw dataset from the versioned folder\n",
    "- Perform basic data quality checks and standardization\n",
    "- Export a cleaned dataset to the processed folder\n",
    "\n",
    "## Inputs\n",
    "\n",
    "- data/raw/v1/environmental_trends.csv\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- data/processed/v1/environmental_trends_clean.csv\n",
    "\n",
    "## Additional Comments\n",
    "\n",
    "- Keep all changes reproducible and logged in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61679ed",
   "metadata": {},
   "source": [
    "## Notebook layout\n",
    "\n",
    "- Section 1: Setup and load raw data\n",
    "- Section 2: Data quality checks\n",
    "- Section 3: Save cleaned output\n",
    "\n",
    "## Business context\n",
    "\n",
    "This notebook supports all downstream analysis and the Streamlit dashboard by ensuring the data foundation is reliable and reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c52972",
   "metadata": {},
   "source": [
    "## Purpose and Context\n",
    "\n",
    "This notebook is the first step in our Global Environmental Trends analysis pipeline. It ensures our data foundation is solid before we draw any conclusions or make predictions.\n",
    "\n",
    "The connection to project guidelines centers on three key areas. For ethics, transparent data quality checks prevent misleading conclusions that could affect public understanding. For communication, clear documentation helps both technical reviewers and non-technical stakeholders understand our process. For project planning, establishing a clean, versioned dataset enables reproducible analysis and future updates.\n",
    "\n",
    "What makes this approach responsible is that we document all quality issues rather than just the ones we fix. We preserve the raw data unchanged by separating raw from processed files. We version outputs so anyone can reproduce our results. We make the cleaning process visible with no hidden transformations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdca7da",
   "metadata": {},
   "source": [
    "## Section 1 - Setup and load raw data\n",
    "\n",
    "This section sets the project root as the working directory and loads the raw CSV for inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc94b54",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39334053",
   "metadata": {},
   "source": [
    "Notebooks are stored in the `jupyter_notebooks/` subfolder. This cell checks the current working directory and navigates to the project root (`global_env_trend/`) if needed, ensuring relative paths to `data/` work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe0af36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\sergi\\\\OneDrive\\\\Documents\\\\Code Institute Data analytics\\\\Capstone project 3\\\\Global_environmental_trends_2000_2024\\\\global_env_trend\\\\jupyter_notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41c2f6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\sergi\\OneDrive\\Documents\\Code Institute Data analytics\\Capstone project 3\\Global_environmental_trends_2000_2024\\global_env_trend\n"
     ]
    }
   ],
   "source": [
    "# Navigate to project root - portable approach that works on any clone\n",
    "current = Path.cwd()\n",
    "\n",
    "if not (current / \"data\" / \"processed\" / \"v1\").exists():\n",
    "    if (current.parent / \"data\" / \"processed\" / \"v1\").exists():\n",
    "        os.chdir(current.parent)\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9abefc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\sergi\\OneDrive\\Documents\\Code Institute Data analytics\\Capstone project 3\\Global_environmental_trends_2000_2024\\global_env_trend\n",
      "Data folder exists: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"Data folder exists: {os.path.exists('data')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f69286c",
   "metadata": {},
   "source": [
    "# Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "346906d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 156 rows and 10 columns\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Country",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Avg_Temperature_degC",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CO2_Emissions_tons_per_capita",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Sea_Level_Rise_mm",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Rainfall_mm",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Population",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Renewable_Energy_pct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Extreme_Weather_Events",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Forest_Area_pct",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "a1b1b22c-5222-4aa0-a738-977ebd1e171b",
       "rows": [
        [
         "0",
         "2000",
         "United States",
         "13.5",
         "20.2",
         "0",
         "715",
         "282500000",
         "6.2",
         "38",
         "33.1"
        ],
        [
         "1",
         "2000",
         "China",
         "12.8",
         "2.7",
         "0",
         "645",
         "1267000000",
         "16.5",
         "24",
         "18.8"
        ],
        [
         "2",
         "2000",
         "Germany",
         "9.3",
         "10.1",
         "0",
         "700",
         "82200000",
         "6.6",
         "12",
         "31.8"
        ],
        [
         "3",
         "2000",
         "Brazil",
         "24.9",
         "1.9",
         "0",
         "1760",
         "175000000",
         "83.7",
         "18",
         "65.4"
        ],
        [
         "4",
         "2000",
         "Australia",
         "21.7",
         "17.2",
         "0",
         "534",
         "19200000",
         "8.8",
         "11",
         "16.2"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Country</th>\n",
       "      <th>Avg_Temperature_degC</th>\n",
       "      <th>CO2_Emissions_tons_per_capita</th>\n",
       "      <th>Sea_Level_Rise_mm</th>\n",
       "      <th>Rainfall_mm</th>\n",
       "      <th>Population</th>\n",
       "      <th>Renewable_Energy_pct</th>\n",
       "      <th>Extreme_Weather_Events</th>\n",
       "      <th>Forest_Area_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>United States</td>\n",
       "      <td>13.5</td>\n",
       "      <td>20.2</td>\n",
       "      <td>0</td>\n",
       "      <td>715</td>\n",
       "      <td>282500000</td>\n",
       "      <td>6.2</td>\n",
       "      <td>38</td>\n",
       "      <td>33.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>China</td>\n",
       "      <td>12.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0</td>\n",
       "      <td>645</td>\n",
       "      <td>1267000000</td>\n",
       "      <td>16.5</td>\n",
       "      <td>24</td>\n",
       "      <td>18.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>Germany</td>\n",
       "      <td>9.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0</td>\n",
       "      <td>700</td>\n",
       "      <td>82200000</td>\n",
       "      <td>6.6</td>\n",
       "      <td>12</td>\n",
       "      <td>31.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>24.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "      <td>1760</td>\n",
       "      <td>175000000</td>\n",
       "      <td>83.7</td>\n",
       "      <td>18</td>\n",
       "      <td>65.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>Australia</td>\n",
       "      <td>21.7</td>\n",
       "      <td>17.2</td>\n",
       "      <td>0</td>\n",
       "      <td>534</td>\n",
       "      <td>19200000</td>\n",
       "      <td>8.8</td>\n",
       "      <td>11</td>\n",
       "      <td>16.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year        Country  Avg_Temperature_degC  CO2_Emissions_tons_per_capita  \\\n",
       "0  2000  United States                  13.5                           20.2   \n",
       "1  2000          China                  12.8                            2.7   \n",
       "2  2000        Germany                   9.3                           10.1   \n",
       "3  2000         Brazil                  24.9                            1.9   \n",
       "4  2000      Australia                  21.7                           17.2   \n",
       "\n",
       "   Sea_Level_Rise_mm  Rainfall_mm  Population  Renewable_Energy_pct  \\\n",
       "0                  0          715   282500000                   6.2   \n",
       "1                  0          645  1267000000                  16.5   \n",
       "2                  0          700    82200000                   6.6   \n",
       "3                  0         1760   175000000                  83.7   \n",
       "4                  0          534    19200000                   8.8   \n",
       "\n",
       "   Extreme_Weather_Events  Forest_Area_pct  \n",
       "0                      38             33.1  \n",
       "1                      24             18.8  \n",
       "2                      12             31.8  \n",
       "3                      18             65.4  \n",
       "4                      11             16.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "raw_path = \"data/raw/v1/environmental_trends.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(raw_path)\n",
    "    print(f\"Loaded {len(df)} rows and {len(df.columns)} columns\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Raw data not found at {raw_path}. Ensure the file exists.\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7fdba7",
   "metadata": {},
   "source": [
    "**What this code does:**\n",
    "\n",
    "Loads the raw CSV from the versioned folder and previews the first rows to verify columns and types before any cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1f5293",
   "metadata": {},
   "source": [
    "## Section 2 - Data quality checks\n",
    "\n",
    "We assess missing values, duplicates, and data types to validate reliability before analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ebf99",
   "metadata": {},
   "source": [
    "# Quality checks\n",
    "\n",
    "**What we're checking and why:**\n",
    "\n",
    "Data quality is the foundation of reliable analysis. Before we can trust any insights or predictions, we need to verify that our data is complete (checking for missing values that could bias results), accurate (reviewing data types to ensure calculations will work correctly), and consistent (identifying duplicates that could skew statistics).\n",
    "\n",
    "From an ethical perspective, poor data quality can lead to misleading conclusions that affect public understanding of climate issues. By documenting these checks, we ensure transparency and accountability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5abba637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Types and Shape ===\n",
      "Shape: (156, 10)\n",
      "\n",
      "Expected columns: Year, Country, Avg_Temperature_degC, CO2_Emissions_tons_per_capita,\n",
      "  Sea_Level_Rise_mm, Rainfall_mm, Population, Renewable_Energy_pct,\n",
      "  Extreme_Weather_Events, Forest_Area_pct\n",
      "\n",
      "Actual columns: ['Year', 'Country', 'Avg_Temperature_degC', 'CO2_Emissions_tons_per_capita', 'Sea_Level_Rise_mm', 'Rainfall_mm', 'Population', 'Renewable_Energy_pct', 'Extreme_Weather_Events', 'Forest_Area_pct']\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156 entries, 0 to 155\n",
      "Data columns (total 10 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   Year                           156 non-null    int64  \n",
      " 1   Country                        156 non-null    object \n",
      " 2   Avg_Temperature_degC           156 non-null    float64\n",
      " 3   CO2_Emissions_tons_per_capita  156 non-null    float64\n",
      " 4   Sea_Level_Rise_mm              156 non-null    int64  \n",
      " 5   Rainfall_mm                    156 non-null    int64  \n",
      " 6   Population                     156 non-null    int64  \n",
      " 7   Renewable_Energy_pct           156 non-null    float64\n",
      " 8   Extreme_Weather_Events         156 non-null    int64  \n",
      " 9   Forest_Area_pct                156 non-null    float64\n",
      "dtypes: float64(4), int64(5), object(1)\n",
      "memory usage: 12.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Data types and non-null counts\n",
    "print(\"=== Data Types and Shape ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nExpected columns: Year, Country, Avg_Temperature_degC, CO2_Emissions_tons_per_capita,\")\n",
    "print(f\"  Sea_Level_Rise_mm, Rainfall_mm, Population, Renewable_Energy_pct,\")\n",
    "print(f\"  Extreme_Weather_Events, Forest_Area_pct\")\n",
    "print(f\"\\nActual columns: {list(df.columns)}\")\n",
    "print()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d86ee4a",
   "metadata": {},
   "source": [
    "**What this shows:**\n",
    "\n",
    "The info method reveals column names and data types (are temperatures stored as numbers?), non-null counts (how many valid values per column?), and memory usage (dataset size). Look for unexpected types such as numbers stored as text or excessive missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb044e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Missing Values ===\n",
      "No missing values found.\n",
      "\n",
      "Total missing cells: 0 out of 1560 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Missing values analysis\n",
    "print(\"=== Missing Values ===\")\n",
    "missing = df.isna().sum().sort_values(ascending=False)\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_report = pd.DataFrame({\"Missing Count\": missing, \"Percentage (%)\": missing_pct})\n",
    "print(missing_report[missing_report[\"Missing Count\"] > 0].to_string() if missing.sum() > 0 else \"No missing values found.\")\n",
    "print(f\"\\nTotal missing cells: {missing.sum()} out of {df.shape[0] * df.shape[1]} ({missing.sum() / (df.shape[0] * df.shape[1]) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a575be6",
   "metadata": {},
   "source": [
    "**Interpreting missing values:**\n",
    "\n",
    "Low counts below 5% are usually acceptable and may indicate incomplete reporting for specific countries or years. High counts above 20% require caution, so consider whether the variable can still be used reliably. Also watch for patterns where missing data is concentrated in certain countries or years, as this may introduce bias.\n",
    "\n",
    "The action we take is to document any columns with significant missing data and note this as a limitation in the final dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc8c2d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Duplicate Analysis ===\n",
      "Fully duplicate rows: 0\n",
      "Duplicate Country-Year pairs: 42\n",
      "\n",
      "Countries with duplicate year entries:\n",
      "Country\n",
      "Australia    [2000, 2000, 2005, 2005, 2010, 2010, 2015, 201...\n",
      "Brazil       [2000, 2000, 2005, 2005, 2010, 2010, 2015, 201...\n",
      "China        [2000, 2000, 2005, 2005, 2010, 2010, 2015, 201...\n",
      "Germany      [2000, 2000, 2005, 2005, 2010, 2010, 2015, 201...\n",
      "India        [2000, 2000, 2005, 2005, 2010, 2010, 2015, 201...\n",
      "Nigeria      [2000, 2000, 2005, 2005, 2010, 2010, 2015, 201...\n",
      "Russia       [2000, 2000, 2005, 2005, 2010, 2010, 2015, 201...\n"
     ]
    }
   ],
   "source": [
    "# Duplicate analysis\n",
    "print(\"=== Duplicate Analysis ===\")\n",
    "n_dupes = df.duplicated().sum()\n",
    "print(f\"Fully duplicate rows: {n_dupes}\")\n",
    "\n",
    "# Check for duplicate Country-Year combinations (more important)\n",
    "n_country_year_dupes = df.duplicated(subset=[\"Country\", \"Year\"]).sum()\n",
    "print(f\"Duplicate Country-Year pairs: {n_country_year_dupes}\")\n",
    "\n",
    "if n_country_year_dupes > 0:\n",
    "    dupe_pairs = df[df.duplicated(subset=[\"Country\", \"Year\"], keep=False)].sort_values([\"Country\", \"Year\"])\n",
    "    print(f\"\\nCountries with duplicate year entries:\")\n",
    "    print(dupe_pairs.groupby(\"Country\")[\"Year\"].apply(list).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225fe11c",
   "metadata": {},
   "source": [
    "**Understanding duplicates:**\n",
    "\n",
    "Duplicate rows can inflate statistics (making trends appear stronger than they are), create misleading visualizations, and violate assumptions in statistical tests.\n",
    "\n",
    "If duplicates exist, we need to investigate whether they're true duplicates (identical entries to remove), valid repeated measurements (keep but document), or data entry errors (needs correction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b204ba6",
   "metadata": {},
   "source": [
    "## Section 3 - Data cleaning\n",
    "\n",
    "We apply cleaning steps to address any issues found in the quality checks: remove duplicates, handle missing values, validate data types and value ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff36be66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning: 156 rows\n",
      "Step 1 - Removed 42 duplicate Country-Year rows → 114 rows remain\n",
      "Step 2 - Removed 0 rows with missing temperature → 114 rows remain\n",
      "\n",
      "Step 4 - Validated data types: Year(int), Population(int)\n",
      "\n",
      "=== Value Range Validation ===\n",
      "  Year: ✓ (expected 1990–2030)\n",
      "  Avg_Temperature_degC: ✓ (expected -20–50)\n",
      "  CO2_Emissions_tons_per_capita: ✓ (expected 0–50)\n",
      "  Sea_Level_Rise_mm: ✓ (expected 0–200)\n",
      "  Rainfall_mm: ✓ (expected 0–5000)\n",
      "  Renewable_Energy_pct: ✓ (expected 0–100)\n",
      "  Forest_Area_pct: ✓ (expected 0–100)\n",
      "  Extreme_Weather_Events: ✓ (expected 0–200)\n",
      "\n",
      "After cleaning: 114 rows, 10 columns\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before cleaning: {len(df)} rows\")\n",
    "\n",
    "# Step 1: Remove duplicate Country-Year pairs (keep first occurrence)\n",
    "before_dupes = len(df)\n",
    "df = df.drop_duplicates(subset=[\"Country\", \"Year\"], keep=\"first\").reset_index(drop=True)\n",
    "print(f\"Step 1 - Removed {before_dupes - len(df)} duplicate Country-Year rows → {len(df)} rows remain\")\n",
    "\n",
    "# Step 2: Drop rows with missing target variable (temperature)\n",
    "before_na = len(df)\n",
    "df = df.dropna(subset=[\"Avg_Temperature_degC\"])\n",
    "print(f\"Step 2 - Removed {before_na - len(df)} rows with missing temperature → {len(df)} rows remain\")\n",
    "\n",
    "# Step 3: Fill remaining missing numeric values with country-level medians\n",
    "numeric_cols = df.select_dtypes(include=\"number\").columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    if df[col].isna().sum() > 0:\n",
    "        filled = df[col].isna().sum()\n",
    "        df[col] = df.groupby(\"Country\")[col].transform(lambda x: x.fillna(x.median()))\n",
    "        # If still missing (country has no data), fill with global median\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "        print(f\"Step 3 - Filled {filled} missing values in {col} with country/global median\")\n",
    "\n",
    "# Step 4: Validate data types\n",
    "df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "df[\"Population\"] = df[\"Population\"].astype(int)\n",
    "print(f\"\\nStep 4 - Validated data types: Year(int), Population(int)\")\n",
    "\n",
    "# Step 5: Value range checks\n",
    "print(f\"\\n=== Value Range Validation ===\")\n",
    "range_checks = {\n",
    "    \"Year\": (1990, 2030),\n",
    "    \"Avg_Temperature_degC\": (-20, 50),\n",
    "    \"CO2_Emissions_tons_per_capita\": (0, 50),\n",
    "    \"Sea_Level_Rise_mm\": (0, 200),\n",
    "    \"Rainfall_mm\": (0, 5000),\n",
    "    \"Renewable_Energy_pct\": (0, 100),\n",
    "    \"Forest_Area_pct\": (0, 100),\n",
    "    \"Extreme_Weather_Events\": (0, 200),\n",
    "}\n",
    "for col, (lo, hi) in range_checks.items():\n",
    "    if col in df.columns:\n",
    "        outliers = ((df[col] < lo) | (df[col] > hi)).sum()\n",
    "        status = \"✓\" if outliers == 0 else f\"⚠ {outliers} out-of-range\"\n",
    "        print(f\"  {col}: {status} (expected {lo}–{hi})\")\n",
    "\n",
    "print(f\"\\nAfter cleaning: {len(df)} rows, {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29178d1",
   "metadata": {},
   "source": [
    "**What the cleaning achieved:**\n",
    "\n",
    "The steps above ensure the dataset is ready for analysis. Here is what each step does in plain English:\n",
    "\n",
    "1. **Duplicate removal** — Some countries had more than one row for the same year (e.g. two entries for Brazil in 2010). We kept only the first occurrence so each Country-Year pair appears exactly once. This prevents inflated statistics and misleading trends.\n",
    "2. **Missing temperature rows dropped** — Temperature is our main variable of interest (the \"target\"). Rows without it cannot contribute to any analysis, so they are removed entirely.\n",
    "3. **Median imputation** — For other numeric columns (CO2, rainfall, etc.), any gaps are filled with the median value for that country. If a country has no data at all for a column, the global median is used as a fallback. Medians are preferred over means because they are less affected by extreme values.\n",
    "4. **Type validation** — Year and Population are converted to whole numbers (integers). This avoids display issues like \"2005.0\" and ensures correct sorting.\n",
    "5. **Range checks** — Each column is checked against sensible boundaries (e.g. temperatures between −20°C and 50°C, percentages between 0% and 100%). Any values outside these ranges are flagged so we can investigate further.\n",
    "\n",
    "After cleaning, the dataset has no missing values, no duplicate Country-Year pairs, and all values fall within expected ranges. This is the single source of truth used by all downstream notebooks and the dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2084f07",
   "metadata": {},
   "source": [
    "## Section 4 - Save cleaned output\n",
    "\n",
    "We store a versioned cleaned dataset for downstream EDA, hypothesis tests, and modeling.\n",
    "\n",
    "**Why versioning matters:** We save to `data/processed/v1/` so future updates go to v2, v3, etc., preserving full history and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef35c803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved 114 rows × 10 columns to data/processed/v1/environmental_trends_clean.csv\n",
      "✓ Countries: 19\n",
      "✓ Year range: 2000 – 2024\n",
      "✓ Missing values: 0\n",
      "✓ Duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "clean_path = \"data/processed/v1/environmental_trends_clean.csv\"\n",
    "df.to_csv(clean_path, index=False)\n",
    "\n",
    "# Validate saved file\n",
    "saved_df = pd.read_csv(clean_path)\n",
    "assert len(saved_df) == len(df), f\"Row count mismatch: saved {len(saved_df)}, expected {len(df)}\"\n",
    "assert list(saved_df.columns) == list(df.columns), \"Column mismatch between saved and cleaned data\"\n",
    "\n",
    "print(f\"✓ Saved {len(saved_df)} rows × {len(saved_df.columns)} columns to {clean_path}\")\n",
    "print(f\"✓ Countries: {saved_df['Country'].nunique()}\")\n",
    "print(f\"✓ Year range: {saved_df['Year'].min()} – {saved_df['Year'].max()}\")\n",
    "print(f\"✓ Missing values: {saved_df.isna().sum().sum()}\")\n",
    "print(f\"✓ Duplicate rows: {saved_df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6384aa58",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Cleaning steps applied:**\n",
    "1. Removed duplicate Country-Year pairs (kept first occurrence)\n",
    "2. Dropped rows with missing temperature (target variable)\n",
    "3. Imputed remaining missing numerics with country-level medians (global median as fallback)\n",
    "4. Validated data types (Year and Population as integers)\n",
    "5. Checked all numeric columns against expected value ranges\n",
    "\n",
    "The cleaned dataset is now saved to `data/processed/v1/environmental_trends_clean.csv` and serves as the single source of truth for all downstream analysis (EDA, hypothesis testing, modeling, and dashboard)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
